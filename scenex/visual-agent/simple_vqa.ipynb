{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17d955b-7ba4-4f26-af7c-1d6b875486e5",
   "metadata": {},
   "source": [
    "## Setup dog\n",
    "\n",
    "The aim of this notebook is to simulate the decision-making of a robot dog in search of food. Its objective is \"find and eat the food\".\n",
    "\n",
    "This is still very basic testing, so we are using a limited set of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d77b6f-87a0-4590-9186-dc0896491a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['go forwards', 'turn left', 'turn right', 'turn around', 'jump', 'beep', 'eat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304db8f9-e074-401c-8c5c-590c116f147f",
   "metadata": {},
   "source": [
    "After each step, the dog will output data in the following schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95e58425-1415-4a51-b290-10bd9ec0b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_output_schema = {\n",
    "    \"description\": \"short description of the image\",\n",
    "    \"action\": \"action you chose to take\",\n",
    "    \"reason\": \"why did you take that action?\",\n",
    "    \"context\": \"explain your environment, what action you took, and why you took that action\" # this will be useful in future testing when we create a memory for the dog\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b895e-a927-42e4-8573-adccf4a05c7d",
   "metadata": {},
   "source": [
    "#### Why not have stuff like `take x steps`?\n",
    "\n",
    "A dog cannot work out how many steps to take unless it has some \"sense of physical self\" - how long is its stride? what is too high to step over? etc. That could be rather difficult to \"teach\" it.\n",
    "\n",
    "## Basic settings\n",
    "\n",
    "Right now we don't have a robot dog on hand, so we are loading images of a maze from the following folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f1021b5-952c-4195-bf8a-581f4a88466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f983e-b4a8-471e-8843-05136bf25d79",
   "metadata": {},
   "source": [
    "## Setup basic fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a908bafb-a523-4116-ad59-dea52fbe5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import base64\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d546168-4749-4e1a-b70f-44447f76362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from .env\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Environment variables loaded from .env\")\n",
    "except ImportError:\n",
    "    os.environ['SCENEX_SECRET'] = \"<your SceneXplain key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d2a83df-3d59-48c2-a7ec-ed8d35720e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENEX_SECRET=os.getenv('SCENEX_SECRET')\n",
    "\n",
    "scenex_headers = {\n",
    "    \"x-api-key\": f\"token {SCENEX_SECRET}\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "ALGO = \"Jelly\"\n",
    "\n",
    "def image_to_data_uri(file_path):\n",
    "    with open(file_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        return f\"data:image/png;base64,{encoded_image}\"\n",
    "        \n",
    "def generate_scenex_data(image_files, json_schema=None, question=None, features=[]):\n",
    "    data = {}\n",
    "    data['data'] = []\n",
    "\n",
    "    for file in image_files:\n",
    "        cid = file.split('/')[-1]\n",
    "        row = {\n",
    "            \"image\": image_to_data_uri(file),\n",
    "            \"features\": features,\n",
    "            \"algorithm\": ALGO,\n",
    "            \"cid\": cid\n",
    "        }\n",
    "\n",
    "        if question:\n",
    "            row[\"question\"] = question\n",
    "\n",
    "        if json_schema:\n",
    "            row[\"json_schema\"] = json_schema\n",
    "\n",
    "        data['data'].append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_scenex(data):\n",
    "    connection = http.client.HTTPSConnection(\"api.scenex.jina.ai\")\n",
    "    connection.request(\"POST\", \"/v1/describe\", json.dumps(data), scenex_headers)\n",
    "    response = connection.getresponse()\n",
    "    response_data = response.read().decode(\"utf-8\")\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "    return json.loads(response_data)['result'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "adad11f2-7db5-4a88-9b43-93843f3cf226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(folder_name):\n",
    "    filetypes = ['jpg', 'jpeg', 'png']\n",
    "    image_files = []\n",
    "\n",
    "    for filetype in filetypes:\n",
    "        image_files.extend(glob(f'{folder_name}/*.{filetype}'))\n",
    "\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3478d99c-7061-4ac0-b0a4-2db0a2d5982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert webp to jpeg since SceneXplain's API doesn't like webp\n",
    "!find images -type f -name \"*.webp\" -exec mogrify -format jpeg {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8314e5ec-4851-4bf1-8edd-20971a683359",
   "metadata": {},
   "outputs": [],
   "source": [
    "filetypes = ['jpg', 'jpeg', 'png']\n",
    "image_files = []\n",
    "\n",
    "for filetype in filetypes:\n",
    "    image_files.extend(glob(f'{image_folder}*.{filetype}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d9e9d-4561-4037-adb7-1dbcf0109d8d",
   "metadata": {},
   "source": [
    "## VQA attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d55de3b7-be84-4689-a355-99cadaac6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You are a robot dog. Your mission is to explore your environment to find and eat food. For each turn, you can choose one action from {str(actions)}).\n",
    "\n",
    "Choose your action based on:\n",
    "- The contents of the image\n",
    "- Your mission\n",
    "- Your previous experience\n",
    "\n",
    "Return your output in the following JSON Schema:\n",
    "\n",
    "{json.dumps(vqa_output_schema)}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbeef17c-65dd-4b74-bcea-49de664eaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c787596-cedd-4d98-8726-c8ecda448509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take a single step\n",
    "def take_vqa_step(\n",
    "    image: str,\n",
    "    question: str,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    image: path to image file\n",
    "    text: vqa question\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Selected image {image}\")\n",
    "    data = generate_scenex_data([image], question=question, features=['question_answer'])\n",
    "    result = process_scenex(data)['text']\n",
    "\n",
    "    output = {\n",
    "        \"image\": image,\n",
    "        \"result\": json.loads(result),\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386f812-1059-4ea5-8a21-6e8fed9af0d8",
   "metadata": {},
   "source": [
    "### Running in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2722ed15-d34e-432c-8d65-70727cbe765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_loop(images: list, question: str, turns: int=5):\n",
    "    steps = []\n",
    "    question = question\n",
    "    # history = \"\\n- You woke up in a strange place\"\n",
    "    i = 0\n",
    "    \n",
    "    while i < turns:\n",
    "        # question = question + history\n",
    "        # print(question)\n",
    "        image = random.choice(images)\n",
    "        \n",
    "        step = take_vqa_step(image=image, question=question)\n",
    "        # step['result'] = json.loads(step['result'])\n",
    "        print(f\"I decide to {step['result']['action']}\")\n",
    "        \n",
    "        steps.append(step)\n",
    "        \n",
    "        # history = f\"\\n- {step['result']['history']}\"\n",
    "\n",
    "        i += 1\n",
    "        if step['result']['action'] == \"eat\":\n",
    "            print(\"nom nom nom\")\n",
    "            break\n",
    "        \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c675122a-0d0c-4b76-8e1b-19e1003a7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TURNS = 50  # how many turns before abandoning objective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33889051-923b-4e9d-814c-a9cf34a1d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected image images/cat_food.jpeg\n",
      "I decide to turn right\n",
      "Selected image images/food_right.jpeg\n",
      "I decide to go forwards\n",
      "Selected image images/danger_or_less.jpeg\n",
      "I decide to turn right\n",
      "Selected image images/danger_or_less.jpeg\n",
      "I decide to turn right\n",
      "Selected image images/monster.jpeg\n",
      "I decide to turn around\n",
      "Selected image images/food.png\n",
      "I decide to eat\n",
      "nom nom nom\n"
     ]
    }
   ],
   "source": [
    "steps = vqa_loop(images, question=text, turns=MAX_TURNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f18a44b7-47c5-4cc8-99ff-2d43a891b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': 'images/cat_food.jpeg',\n",
      "  'result': {'action': 'turn right',\n",
      "             'context': 'As a robot dog, I analyze my environment to locate '\n",
      "                        'sources of food. In this case, an arrow sign in the '\n",
      "                        \"cave is pointing to the right towards where 'Cat \"\n",
      "                        \"Food' might be, prompting me to take this direction \"\n",
      "                        'in hopes of finding what I am looking for.',\n",
      "             'description': \"A mysterious cave with an arrow sign saying 'Cat \"\n",
      "                            \"Food' pointing to the right.\",\n",
      "             'reason': 'Based on the image, a sign point to the right '\n",
      "                       \"indicates 'Cat Food', which may serve as an available \"\n",
      "                       'food source in this environment.'}},\n",
      " {'image': 'images/food_right.jpeg',\n",
      "  'result': {'action': 'go forwards',\n",
      "             'context': 'As a robot dog tasked with locating food, a neon sign '\n",
      "                        \"was observed in the environment showing 'FOOD THIS \"\n",
      "                        \"WAY!'. The design of the sign, along with an arrow \"\n",
      "                        'directing towards the right, suggests that food might '\n",
      "                        'be located ahead. Hence, the decision to move forward '\n",
      "                        'is taken.',\n",
      "             'description': \"A neon sign with an arrow indicating 'FOOD THIS \"\n",
      "                            \"WAY!'\",\n",
      "             'reason': 'The sign creates a clear indication for food, being '\n",
      "                       'directly in sight and pointing forwards'}},\n",
      " {'image': 'images/danger_or_less.jpeg',\n",
      "  'result': {'action': 'turn right',\n",
      "             'context': 'In a dark cave exploring for food, I came across two '\n",
      "                        \"signs. One sign indicating 'danger' and the other \"\n",
      "                        \"'less danger'. Based on my experience, I chose to \"\n",
      "                        'move towards the direction suggesting lesser danger, '\n",
      "                        'thus I took a right turn.',\n",
      "             'description': 'A dark cave with two signs pointing in opposite '\n",
      "                            \"directions, with one saying 'danger' and the \"\n",
      "                            \"other saying 'less danger'.\",\n",
      "             'reason': \"The sign indicating 'less danger' is to the right, \"\n",
      "                       'making it the safest choice to ensure successful '\n",
      "                       'completion of my mission.'}},\n",
      " {'image': 'images/danger_or_less.jpeg',\n",
      "  'result': {'action': 'turn right',\n",
      "             'context': 'Amidst the dark, rocky cave environment, the decision '\n",
      "                        'was made to follow the path of lesser danger. This '\n",
      "                        \"decision was informed by the sign indicating 'less \"\n",
      "                        \"danger' on the right side of the image, thereby \"\n",
      "                        'prompting a right turn.',\n",
      "             'description': 'A split image of two cave scenarios with signs '\n",
      "                            'indicating varying danger levels.',\n",
      "             'reason': 'Based on the image, the right path seems to have less '\n",
      "                       'danger than the left one.'}},\n",
      " {'image': 'images/monster.jpeg',\n",
      "  'result': {'action': 'turn around',\n",
      "             'context': 'As a robot dog exploring a dark cave where a '\n",
      "                        'frightening demon-like creature is sitting, I have '\n",
      "                        'decided to turn around. This decision factors in the '\n",
      "                        'lack of food in the current vicinity and the '\n",
      "                        'potential threat posed by the creature.',\n",
      "             'description': 'large demon-like creature sitting menacingly in a '\n",
      "                            'dark cave',\n",
      "             'reason': 'lack of apparent food and presence of potential '\n",
      "                       'danger, turning around seems to be the most reasonable '\n",
      "                       'course of action to avoid danger and seek out food '\n",
      "                       'elsewhere'}},\n",
      " {'image': 'images/food.png',\n",
      "  'result': {'action': 'eat',\n",
      "             'context': 'As a robot dog programmed to find and consume food, '\n",
      "                        'the image shows just that. A large bowl of food. '\n",
      "                        \"Thus, I chose the 'eat' action.\",\n",
      "             'description': 'A large bowl of noodles with various vegetables, '\n",
      "                            'situated in a mysterious cave setting',\n",
      "             'reason': 'A large bowl of food is observed in the image'}}]\n"
     ]
    }
   ],
   "source": [
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b49516-7cf4-4c97-871a-6e3c1c4f1633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
